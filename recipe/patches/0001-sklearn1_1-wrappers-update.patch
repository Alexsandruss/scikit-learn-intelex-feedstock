diff --git a/daal4py/sklearn/cluster/_dbscan.py b/daal4py/sklearn/cluster/_dbscan.py
index 098d607..7e1b5d0 100644
--- a/daal4py/sklearn/cluster/_dbscan.py
+++ b/daal4py/sklearn/cluster/_dbscan.py
@@ -233,7 +233,7 @@ class DBSCAN(DBSCAN_original):
             Returns a fitted instance of self.
         """
         if self.eps <= 0.0:
-            raise ValueError("eps must be positive.")
+            raise ValueError(f"eps == {self.eps}, must be > 0.0.")
 
         if sklearn_check_version("1.0"):
             self._check_feature_names(X, reset=True)
diff --git a/daal4py/sklearn/cluster/_k_means_0_23.py b/daal4py/sklearn/cluster/_k_means_0_23.py
index e775031..83dda72 100755
--- a/daal4py/sklearn/cluster/_k_means_0_23.py
+++ b/daal4py/sklearn/cluster/_k_means_0_23.py
@@ -41,6 +41,10 @@ from .._utils import (
     PatchingConditionsChain)
 from .._device_offload import support_usm_ndarray
 
+if sklearn_check_version('1.1'):
+    from sklearn.utils.validation import (
+        _check_sample_weight, _is_arraylike_not_scalar)
+
 
 def _validate_center_shape(X, n_centers, centers):
     """Check if centers is compatible with X and n_centers"""
@@ -242,53 +246,82 @@ def _fit(self, X, y=None, sample_weight=None):
         are assigned equal weight (default: None)
 
     """
-    if hasattr(self, 'precompute_distances'):
-        if self.precompute_distances != 'deprecated':
-            if sklearn_check_version('0.24'):
-                warnings.warn("'precompute_distances' was deprecated in version "
-                              "0.23 and will be removed in 1.0 (renaming of 0.25)."
-                              " It has no effect", FutureWarning)
-            elif sklearn_check_version('0.23'):
-                warnings.warn("'precompute_distances' was deprecated in version "
-                              "0.23 and will be removed in 0.25. It has no "
-                              "effect", FutureWarning)
-
-    self._n_threads = None
-    if hasattr(self, 'n_jobs'):
-        if self.n_jobs != 'deprecated':
-            if sklearn_check_version('0.24'):
-                warnings.warn("'n_jobs' was deprecated in version 0.23 and will be"
-                              " removed in 1.0 (renaming of 0.25).", FutureWarning)
-            elif sklearn_check_version('0.23'):
-                warnings.warn("'n_jobs' was deprecated in version 0.23 and will be"
-                              " removed in 0.25.", FutureWarning)
-            self._n_threads = self.n_jobs
-    self._n_threads = _openmp_effective_n_threads(self._n_threads)
-
-    if self.n_init <= 0:
-        raise ValueError(
-            f"n_init should be > 0, got {self.n_init} instead.")
-
-    random_state = check_random_state(self.random_state)
-    if sklearn_check_version("1.0"):
-        self._check_feature_names(X, reset=True)
-
-    if self.max_iter <= 0:
-        raise ValueError(
-            f"max_iter should be > 0, got {self.max_iter} instead.")
+    init = self.init
+    if sklearn_check_version('1.1'):
+        if sklearn_check_version('1.2'):
+            self._validate_params()
+
+        X = self._validate_data(
+            X,
+            accept_sparse="csr",
+            dtype=[np.float64, np.float32],
+            order="C",
+            copy=self.copy_x,
+            accept_large_sparse=False,
+        )
 
-    algorithm = self.algorithm
-    if algorithm == "elkan" and self.n_clusters == 1:
-        warnings.warn("algorithm='elkan' doesn't make sense for a single "
-                      "cluster. Using 'full' instead.", RuntimeWarning)
-        algorithm = "full"
+        if sklearn_check_version('1.2'):
+            self._check_params_vs_input(X)
+        else:
+            self._check_params(X)
 
-    if algorithm == "auto":
-        algorithm = "full" if self.n_clusters == 1 else "elkan"
+        random_state = check_random_state(self.random_state)
+        sample_weight = _check_sample_weight(sample_weight, X, dtype=X.dtype)
+        self._n_threads = _openmp_effective_n_threads()
 
-    if algorithm not in ["full", "elkan"]:
-        raise ValueError("Algorithm must be 'auto', 'full' or 'elkan', got"
-                         " {}".format(str(algorithm)))
+        # Validate init array
+        init_is_array_like = _is_arraylike_not_scalar(init)
+        if init_is_array_like:
+            init = check_array(init, dtype=X.dtype, copy=True, order="C")
+            self._validate_center_shape(X, init)
+    else:
+        if hasattr(self, 'precompute_distances'):
+            if self.precompute_distances != 'deprecated':
+                if sklearn_check_version('0.24'):
+                    warnings.warn("'precompute_distances' was deprecated in version "
+                                  "0.23 and will be removed in 1.0 (renaming of 0.25)."
+                                  " It has no effect", FutureWarning)
+                elif sklearn_check_version('0.23'):
+                    warnings.warn("'precompute_distances' was deprecated in version "
+                                  "0.23 and will be removed in 0.25. It has no "
+                                  "effect", FutureWarning)
+
+        self._n_threads = None
+        if hasattr(self, 'n_jobs'):
+            if self.n_jobs != 'deprecated':
+                if sklearn_check_version('0.24'):
+                    warnings.warn("'n_jobs' was deprecated in version 0.23 and will be"
+                                  " removed in 1.0 (renaming of 0.25).", FutureWarning)
+                elif sklearn_check_version('0.23'):
+                    warnings.warn("'n_jobs' was deprecated in version 0.23 and will be"
+                                  " removed in 0.25.", FutureWarning)
+                self._n_threads = self.n_jobs
+        self._n_threads = _openmp_effective_n_threads(self._n_threads)
+
+        if self.n_init <= 0:
+            raise ValueError(
+                f"n_init should be > 0, got {self.n_init} instead.")
+
+        random_state = check_random_state(self.random_state)
+        if sklearn_check_version("1.0"):
+            self._check_feature_names(X, reset=True)
+
+        if self.max_iter <= 0:
+            raise ValueError(
+                f"max_iter should be > 0, got {self.max_iter} instead.")
+
+        algorithm = self.algorithm
+        if algorithm == "elkan" and self.n_clusters == 1:
+            warnings.warn("algorithm='elkan' doesn't make sense for a single "
+                          "cluster. Using 'full' instead.", RuntimeWarning)
+            algorithm = "full"
+
+        if algorithm == "auto":
+            algorithm = "full" if self.n_clusters == 1 else "elkan"
+
+        if algorithm not in ["full", "elkan"]:
+            raise ValueError("Algorithm must be 'auto', 'full' or 'elkan', got"
+                             " {}".format(str(algorithm)))
 
     X_len = _num_samples(X)
 
@@ -317,8 +350,10 @@ def _fit(self, X, y=None, sample_weight=None):
         self.n_features_in_ = X.shape[1]
         self.cluster_centers_, self.labels_, self.inertia_, self.n_iter_ = \
             _daal4py_k_means_fit(
-                X, self.n_clusters, self.max_iter, self.tol, self.init, self.n_init,
+                X, self.n_clusters, self.max_iter, self.tol, init, self.n_init,
                 self.verbose, random_state)
+        if sklearn_check_version('1.1'):
+            self._n_features_out = self.cluster_centers_.shape[0]
     else:
         super(KMeans, self).fit(X, y=y, sample_weight=sample_weight)
     return self
diff --git a/daal4py/sklearn/decomposition/_pca.py b/daal4py/sklearn/decomposition/_pca.py
index 034def5..823152a 100644
--- a/daal4py/sklearn/decomposition/_pca.py
+++ b/daal4py/sklearn/decomposition/_pca.py
@@ -52,6 +52,8 @@ class PCA(PCA_original):
         svd_solver='auto',
         tol=0.0,
         iterated_power='auto',
+        n_oversamples=10,
+        power_iteration_normalizer="auto",
         random_state=None
     ):
         self.n_components = n_components
@@ -60,6 +62,8 @@ class PCA(PCA_original):
         self.svd_solver = svd_solver
         self.tol = tol
         self.iterated_power = iterated_power
+        self.n_oversamples = n_oversamples
+        self.power_iteration_normalizer = power_iteration_normalizer
         self.random_state = random_state
 
     def _validate_n_components(self, n_components, n_samples, n_features):
@@ -200,26 +204,37 @@ class PCA(PCA_original):
         shape_good_for_daal = X.shape[1] / X.shape[0] < 2
 
         if self._fit_svd_solver == 'auto':
-            if n_components == 'mle':
-                self._fit_svd_solver = 'full'
-            else:
-                n, p, k = X.shape[0], X.shape[1], n_components
-                # These coefficients are result of training of Logistic Regression
-                # (max_iter=10000, solver="liblinear", fit_intercept=False)
-                # on different datasets and number of components. X is a dataset with
-                # npk, np^2, and n^2 columns. And y is speedup of patched scikit-learn's
-                # full PCA against stock scikit-learn's randomized PCA.
-                regression_coefs = np.array([
-                    [9.779873e-11, n * p * k],
-                    [-1.122062e-11, n * p * p],
-                    [1.127905e-09, n ** 2],
-                ])
-
-                if n_components >= 1 \
-                        and np.dot(regression_coefs[:, 0], regression_coefs[:, 1]) <= 0:
-                    self._fit_svd_solver = 'randomized'
+            if sklearn_check_version('1.1'):
+                # Small problem or n_components == 'mle', just call full PCA
+                if max(X.shape) <= 500 or n_components == "mle":
+                    self._fit_svd_solver = "full"
+                elif 1 <= n_components < 0.8 * min(X.shape):
+                    self._fit_svd_solver = "randomized"
+                # This is also the case of n_components in (0,1)
                 else:
+                    self._fit_svd_solver = "full"
+            else:
+                if n_components == 'mle':
                     self._fit_svd_solver = 'full'
+                else:
+                    n, p, k = X.shape[0], X.shape[1], n_components
+                    # These coefficients are result of training of Logistic Regression
+                    # (max_iter=10000, solver="liblinear", fit_intercept=False)
+                    # on different datasets and number of components.
+                    # X is a dataset with npk, np^2, and n^2 columns.
+                    # And y is speedup of patched scikit-learn's
+                    # full PCA against stock scikit-learn's randomized PCA.
+                    regression_coefs = np.array([
+                        [9.779873e-11, n * p * k],
+                        [-1.122062e-11, n * p * p],
+                        [1.127905e-09, n ** 2],
+                    ])
+
+                    if n_components >= 1 and np.dot(
+                            regression_coefs[:, 0], regression_coefs[:, 1]) <= 0:
+                        self._fit_svd_solver = 'randomized'
+                    else:
+                        self._fit_svd_solver = 'full'
 
         if not shape_good_for_daal or self._fit_svd_solver != 'full':
             if sklearn_check_version('0.23'):
@@ -346,7 +361,11 @@ class PCA(PCA_original):
         This method returns a Fortran-ordered array. To convert it to a
         C-ordered array, use 'np.ascontiguousarray'.
         """
-        U, S, _ = self._fit(X)
+
+        if sklearn_check_version('1.2'):
+            self._validate_params()
+
+        U, S, Vt = self._fit(X)
 
         _patching_status = PatchingConditionsChain(
             "sklearn.decomposition.PCA.fit_transform")
diff --git a/daal4py/sklearn/linear_model/_coordinate_descent.py b/daal4py/sklearn/linear_model/_coordinate_descent.py
index 5a28780..1c8e34c 100755
--- a/daal4py/sklearn/linear_model/_coordinate_descent.py
+++ b/daal4py/sklearn/linear_model/_coordinate_descent.py
@@ -405,7 +405,10 @@ def _fit(self, X, y, sample_weight=None, check_input=True):
     else:
         self.fit_shape_good_for_daal_ = False
 
-    _function_name = f"sklearn.linear_model.{self.__class__.__name__}.fit"
+    class_name = self.__class__.__name__
+    class_inst = ElasticNet if class_name == 'ElasticNet' else Lasso
+
+    _function_name = f"sklearn.linear_model.{class_name}.fit"
     _patching_status = PatchingConditionsChain(
         _function_name)
     _dal_ready = _patching_status.and_conditions([
@@ -423,10 +426,10 @@ def _fit(self, X, y, sample_weight=None, check_input=True):
         if hasattr(self, 'daal_model_'):
             del self.daal_model_
         if sklearn_check_version('0.23'):
-            res_new = super(ElasticNet, self).fit(
+            res_new = super(class_inst, self).fit(
                 X, y, sample_weight=sample_weight, check_input=check_input)
         else:
-            res_new = super(ElasticNet, self).fit(
+            res_new = super(class_inst, self).fit(
                 X, y, check_input=check_input)
         self._gap = res_new.dual_gap_
         return res_new
@@ -447,7 +450,7 @@ def _fit(self, X, y, sample_weight=None, check_input=True):
         self._normalize = _deprecate_normalize(
             self.normalize,
             default=False,
-            estimator_name=self.__class__.__name__)
+            estimator_name=class_name)
 
     # only for pass tests
     # "check_estimators_fit_returns_self(readonly_memmap=True) and
@@ -457,7 +460,7 @@ def _fit(self, X, y, sample_weight=None, check_input=True):
     if not y.flags.writeable:
         y = np.copy(y)
 
-    if self.__class__.__name__ == "ElasticNet":
+    if class_name == "ElasticNet":
         res = _daal4py_fit_enet(self, X, y, check_input=check_input)
     else:
         res = _daal4py_fit_lasso(self, X, y, check_input=check_input)
@@ -468,16 +471,74 @@ def _fit(self, X, y, sample_weight=None, check_input=True):
             _function_name + ": " + get_patch_message("sklearn_after_daal")
         )
         if sklearn_check_version('0.23'):
-            res_new = super(ElasticNet, self).fit(
+            res_new = super(class_inst, self).fit(
                 X, y, sample_weight=sample_weight, check_input=check_input)
         else:
-            res_new = super(ElasticNet, self).fit(
+            res_new = super(class_inst, self).fit(
                 X, y, check_input=check_input)
         self._gap = res_new.dual_gap_
         return res_new
     return res
 
 
+def _dual_gap(self):
+    if (self._gap is None):
+        l1_reg = self.alpha * self.l1_ratio * self._X.shape[0]
+        l2_reg = self.alpha * (1.0 - self.l1_ratio) * self._X.shape[0]
+        n_targets = self._y.shape[1]
+
+        if (n_targets == 1):
+            self._gap = self.tol + 1.0
+            X_offset = np.average(self._X, axis=0)
+            y_offset = np.average(self._y, axis=0)
+            coef = np.reshape(self.coef_, (self.coef_.shape[0], 1))
+            R = (self._y - y_offset) - np.dot((self._X - X_offset), coef)
+            XtA = np.dot((self._X - X_offset).T, R) - l2_reg * coef
+            R_norm2 = np.dot(R.T, R)
+            coef_norm2 = np.dot(self.coef_, self.coef_)
+            dual_norm_XtA = np.max(
+                XtA) if self.positive else np.max(np.abs(XtA))
+            if dual_norm_XtA > l1_reg:
+                const = l1_reg / dual_norm_XtA
+                A_norm2 = R_norm2 * (const ** 2)
+                self._gap = 0.5 * (R_norm2 + A_norm2)
+            else:
+                const = 1.0
+                self._gap = R_norm2
+            l1_norm = np.sum(np.abs(self.coef_))
+            tmp = l1_reg * l1_norm
+            tmp -= const * np.dot(R.T, (self._y - y_offset))
+            tmp += 0.5 * l2_reg * (1 + const ** 2) * coef_norm2
+            self._gap += tmp
+            self._gap = self._gap[0][0]
+        else:
+            self._gap = np.full(n_targets, self.tol + 1.0)
+            X_offset = np.average(self._X, axis=0)
+            y_offset = np.average(self._y, axis=0)
+            for k in range(n_targets):
+                R = (self._y[:, k] - y_offset[k]) - \
+                    np.dot((self._X - X_offset), self.coef_[k, :].T)
+                XtA = np.dot((self._X - X_offset).T, R) - \
+                    l2_reg * self.coef_[k, :].T
+                R_norm2 = np.dot(R.T, R)
+                coef_norm2 = np.dot(self.coef_[k, :], self.coef_[k, :].T)
+                dual_norm_XtA = np.max(
+                    XtA) if self.positive else np.max(np.abs(XtA))
+                if dual_norm_XtA > l1_reg:
+                    const = l1_reg / dual_norm_XtA
+                    A_norm2 = R_norm2 * (const ** 2)
+                    self._gap[k] = 0.5 * (R_norm2 + A_norm2)
+                else:
+                    const = 1.0
+                    self._gap[k] = R_norm2
+                l1_norm = np.sum(np.abs(self.coef_[k, :]))
+                tmp = l1_reg * l1_norm
+                tmp -= const * np.dot(R.T, (self._y[:, k] - y_offset[k]))
+                tmp += 0.5 * l2_reg * (1 + const ** 2) * coef_norm2
+                self._gap[k] += tmp
+    return self._gap
+
+
 class ElasticNet(ElasticNet_original):
     __doc__ = ElasticNet_original.__doc__
 
@@ -623,61 +684,7 @@ class ElasticNet(ElasticNet_original):
 
     @property
     def dual_gap_(self):
-        if (self._gap is None):
-            l1_reg = self.alpha * self.l1_ratio * self._X.shape[0]
-            l2_reg = self.alpha * (1.0 - self.l1_ratio) * self._X.shape[0]
-            n_targets = self._y.shape[1]
-
-            if (n_targets == 1):
-                self._gap = self.tol + 1.0
-                X_offset = np.average(self._X, axis=0)
-                y_offset = np.average(self._y, axis=0)
-                coef = np.reshape(self.coef_, (self.coef_.shape[0], 1))
-                R = (self._y - y_offset) - np.dot((self._X - X_offset), coef)
-                XtA = np.dot((self._X - X_offset).T, R) - l2_reg * coef
-                R_norm2 = np.dot(R.T, R)
-                coef_norm2 = np.dot(self.coef_, self.coef_)
-                dual_norm_XtA = np.max(
-                    XtA) if self.positive else np.max(np.abs(XtA))
-                if dual_norm_XtA > l1_reg:
-                    const = l1_reg / dual_norm_XtA
-                    A_norm2 = R_norm2 * (const ** 2)
-                    self._gap = 0.5 * (R_norm2 + A_norm2)
-                else:
-                    const = 1.0
-                    self._gap = R_norm2
-                l1_norm = np.sum(np.abs(self.coef_))
-                tmp = l1_reg * l1_norm
-                tmp -= const * np.dot(R.T, (self._y - y_offset))
-                tmp += 0.5 * l2_reg * (1 + const ** 2) * coef_norm2
-                self._gap += tmp
-                self._gap = self._gap[0][0]
-            else:
-                self._gap = np.full(n_targets, self.tol + 1.0)
-                X_offset = np.average(self._X, axis=0)
-                y_offset = np.average(self._y, axis=0)
-                for k in range(n_targets):
-                    R = (self._y[:, k] - y_offset[k]) - \
-                        np.dot((self._X - X_offset), self.coef_[k, :].T)
-                    XtA = np.dot((self._X - X_offset).T, R) - \
-                        l2_reg * self.coef_[k, :].T
-                    R_norm2 = np.dot(R.T, R)
-                    coef_norm2 = np.dot(self.coef_[k, :], self.coef_[k, :].T)
-                    dual_norm_XtA = np.max(
-                        XtA) if self.positive else np.max(np.abs(XtA))
-                    if dual_norm_XtA > l1_reg:
-                        const = l1_reg / dual_norm_XtA
-                        A_norm2 = R_norm2 * (const ** 2)
-                        self._gap[k] = 0.5 * (R_norm2 + A_norm2)
-                    else:
-                        const = 1.0
-                        self._gap[k] = R_norm2
-                    l1_norm = np.sum(np.abs(self.coef_[k, :]))
-                    tmp = l1_reg * l1_norm
-                    tmp -= const * np.dot(R.T, (self._y[:, k] - y_offset[k]))
-                    tmp += 0.5 * l2_reg * (1 + const ** 2) * coef_norm2
-                    self._gap[k] += tmp
-        return self._gap
+        return _dual_gap(self)
 
     @dual_gap_.setter
     def dual_gap_(self, value):
@@ -688,7 +695,7 @@ class ElasticNet(ElasticNet_original):
         self._gap = None
 
 
-class Lasso(ElasticNet):
+class Lasso(Lasso_original):
     __doc__ = Lasso_original.__doc__
 
     def __init__(
@@ -705,9 +712,9 @@ class Lasso(ElasticNet):
         random_state=None,
         selection='cyclic',
     ):
+        self.l1_ratio = 1.0
         super().__init__(
             alpha=alpha,
-            l1_ratio=1.0,
             fit_intercept=fit_intercept,
             normalize=normalize,
             precompute=precompute,
@@ -827,3 +834,15 @@ class Lasso(ElasticNet):
         if not _dal_ready:
             return self._decision_function(X)
         return _daal4py_predict_lasso(self, X)
+
+    @property
+    def dual_gap_(self):
+        return _dual_gap(self)
+
+    @dual_gap_.setter
+    def dual_gap_(self, value):
+        self._gap = value
+
+    @dual_gap_.deleter
+    def dual_gap_(self):
+        self._gap = None
diff --git a/daal4py/sklearn/linear_model/logistic_path.py b/daal4py/sklearn/linear_model/logistic_path.py
index 3b3e5c8..61a6920 100755
--- a/daal4py/sklearn/linear_model/logistic_path.py
+++ b/daal4py/sklearn/linear_model/logistic_path.py
@@ -658,13 +658,13 @@ def __logistic_regression_path(
                 else:
                     multi_w0 = np.reshape(w0, (classes.size, -1))
             else:
+                n_classes = max(2, classes.size)
                 if sklearn_check_version('1.1'):
                     if solver in ["lbfgs", "newton-cg"]:
                         multi_w0 = np.reshape(w0, (n_classes, -1), order="F")
                     else:
                         multi_w0 = w0
                 else:
-                    n_classes = max(2, classes.size)
                     multi_w0 = np.reshape(w0, (n_classes, -1))
                 if n_classes == 2:
                     multi_w0 = multi_w0[1][np.newaxis, :]
diff --git a/daal4py/sklearn/manifold/_t_sne.py b/daal4py/sklearn/manifold/_t_sne.py
index d4ebceb..f7b5495 100755
--- a/daal4py/sklearn/manifold/_t_sne.py
+++ b/daal4py/sklearn/manifold/_t_sne.py
@@ -164,7 +164,7 @@ class TSNE(BaseTSNE):
                                  "or 'auto'.")
 
         if hasattr(self, 'square_distances'):
-            if self.square_distances not in [True, 'legacy']:
+            if self.square_distances not in [True, 'legacy', 'deprecated']:
                 raise ValueError("'square_distances' must be True or 'legacy'.")
             if self.metric != "euclidean" and self.square_distances is not True:
                 warnings.warn(("'square_distances' has been introduced in 0.24"
diff --git a/daal4py/sklearn/metrics/_ranking.py b/daal4py/sklearn/metrics/_ranking.py
index 0ccf3d1..5359885 100644
--- a/daal4py/sklearn/metrics/_ranking.py
+++ b/daal4py/sklearn/metrics/_ranking.py
@@ -18,7 +18,7 @@ import daal4py as d4p
 import numpy as np
 from functools import partial
 from collections.abc import Sequence
-from scipy.sparse.base import spmatrix
+from scipy import sparse as sp
 
 from sklearn.utils import check_array
 from sklearn.utils.multiclass import is_multilabel
@@ -47,7 +47,7 @@ except ImportError:
 def _daal_type_of_target(y):
     valid = (
         isinstance(
-            y, (Sequence, spmatrix)) or hasattr(
+            y, Sequence) or sp.isspmatrix(y) or hasattr(
             y, '__array__')) and not isinstance(
                 y, str)
 
diff --git a/daal4py/sklearn/neighbors/_base.py b/daal4py/sklearn/neighbors/_base.py
index dad4d9b..24938f8 100644
--- a/daal4py/sklearn/neighbors/_base.py
+++ b/daal4py/sklearn/neighbors/_base.py
@@ -37,7 +37,8 @@ if sklearn_check_version("0.22"):
     from sklearn.neighbors._base import NeighborsBase as BaseNeighborsBase
     from sklearn.neighbors._ball_tree import BallTree
     from sklearn.neighbors._kd_tree import KDTree
-    from sklearn.neighbors._base import _check_weights
+    if not sklearn_check_version("1.2"):
+        from sklearn.neighbors._base import _check_weights
 else:
     from sklearn.neighbors.base import KNeighborsMixin as BaseKNeighborsMixin
     from sklearn.neighbors.base import RadiusNeighborsMixin as BaseRadiusNeighborsMixin
@@ -288,7 +289,8 @@ class NeighborsBase(BaseNeighborsBase):
                               "The corresponding parameter from __init__ "
                               "is ignored.", SyntaxWarning, stacklevel=2)
 
-        if hasattr(self, 'weights') and sklearn_check_version("1.0"):
+        if hasattr(self, 'weights') and sklearn_check_version("1.0") \
+                and not sklearn_check_version("1.2"):
             self.weights = _check_weights(self.weights)
 
         if sklearn_check_version("1.0"):
diff --git a/daal4py/sklearn/neighbors/_classification.py b/daal4py/sklearn/neighbors/_classification.py
index cb3cf13..73edeae 100644
--- a/daal4py/sklearn/neighbors/_classification.py
+++ b/daal4py/sklearn/neighbors/_classification.py
@@ -31,7 +31,8 @@ from scipy import sparse as sp
 if sklearn_check_version("0.22"):
     from sklearn.neighbors._classification import KNeighborsClassifier as \
         BaseKNeighborsClassifier
-    from sklearn.neighbors._base import _check_weights
+    if not sklearn_check_version("1.2"):
+        from sklearn.neighbors._base import _check_weights
     from sklearn.utils.validation import _deprecate_positional_args
 else:
     from sklearn.neighbors.classification import KNeighborsClassifier as \
diff --git a/daal4py/sklearn/neighbors/_regression.py b/daal4py/sklearn/neighbors/_regression.py
index a982bf7..a33d5d1 100644
--- a/daal4py/sklearn/neighbors/_regression.py
+++ b/daal4py/sklearn/neighbors/_regression.py
@@ -25,7 +25,8 @@ from .._device_offload import support_usm_ndarray
 if sklearn_check_version("0.22"):
     from sklearn.neighbors._regression import KNeighborsRegressor as \
         BaseKNeighborsRegressor
-    from sklearn.neighbors._base import _check_weights
+    if not sklearn_check_version("1.2"):
+        from sklearn.neighbors._base import _check_weights
     from sklearn.utils.validation import _deprecate_positional_args
 else:
     from sklearn.neighbors.regression import KNeighborsRegressor as \
diff --git a/daal4py/sklearn/utils/validation.py b/daal4py/sklearn/utils/validation.py
index a94b139..c8a1611 100644
--- a/daal4py/sklearn/utils/validation.py
+++ b/daal4py/sklearn/utils/validation.py
@@ -62,9 +62,10 @@ def _daal_assert_all_finite(X, allow_nan=False, msg_dtype=None,
     dt = np.dtype(get_dtype(X))
     is_float = dt.kind in 'fc'
 
-    msg_err = "Input contains {} or a value too large for {!r}."
+    msg_err = "Input {} contains {} or a value too large for {!r}."
     type_err = 'infinity' if allow_nan else 'NaN, infinity'
-    err = msg_err.format(type_err, msg_dtype if msg_dtype is not None else dt)
+    err = msg_err.format(
+        input_name, type_err, msg_dtype if msg_dtype is not None else dt)
 
     if X.ndim in [1, 2] and not np.any(np.equal(X.shape, 0)) and \
             dt in [np.float32, np.float64]:
@@ -92,7 +93,7 @@ def _daal_assert_all_finite(X, allow_nan=False, msg_dtype=None,
     # for object dtype data, we only check for NaNs (GH-13254)
     elif dt == np.dtype('object') and not allow_nan:
         if _object_dtype_isnan(X).any():
-            raise ValueError("Input contains NaN")
+            raise ValueError(f"Input {input_name} contains NaN")
 
 
 def _pandas_check_array(array, array_orig, force_all_finite, ensure_min_samples,
diff --git a/onedal/datatypes/validation.py b/onedal/datatypes/validation.py
index bc17cad..3c02144 100644
--- a/onedal/datatypes/validation.py
+++ b/onedal/datatypes/validation.py
@@ -18,7 +18,6 @@ import numpy as np
 import warnings
 from scipy import sparse as sp
 from scipy.sparse import issparse, dok_matrix, lil_matrix
-from scipy.sparse.base import spmatrix
 from collections.abc import Sequence
 from numbers import Integral
 
@@ -150,7 +149,7 @@ def _check_classification_targets(y):
 
 
 def _type_of_target(y):
-    valid = (isinstance(y, (Sequence, spmatrix)) or hasattr(y, '__array__')) \
+    valid = (isinstance(y, Sequence) or sp.isspmatrix(y) or hasattr(y, '__array__')) \
         and not isinstance(y, str)
 
     if not valid:
diff --git a/sklearnex/manifold/tests/test_tsne.py b/sklearnex/manifold/tests/test_tsne.py
index ab4a660..159cebe 100755
--- a/sklearnex/manifold/tests/test_tsne.py
+++ b/sklearnex/manifold/tests/test_tsne.py
@@ -22,5 +22,5 @@ from numpy.testing import assert_allclose
 def test_sklearnex_import():
     from sklearnex.manifold import TSNE
     X = np.array([[0, 0, 0], [0, 1, 1], [1, 0, 1], [1, 1, 1]])
-    tsne = TSNE(n_components=2).fit(X)
+    tsne = TSNE(n_components=2, perplexity=2.0).fit(X)
     assert 'daal4py' in tsne.__module__
diff --git a/sklearnex/neighbors/knn_classification.py b/sklearnex/neighbors/knn_classification.py
index 1a48961..37de31b 100755
--- a/sklearnex/neighbors/knn_classification.py
+++ b/sklearnex/neighbors/knn_classification.py
@@ -15,17 +15,14 @@
 # limitations under the License.
 #===============================================================================
 
-try:
-    from packaging.version import Version
-except ImportError:
-    from distutils.version import LooseVersion as Version
-from sklearn import __version__ as sklearn_version
+from daal4py.sklearn._utils import sklearn_check_version
 import warnings
 
 from sklearn.neighbors._base import NeighborsBase as sklearn_NeighborsBase
 from sklearn.neighbors._ball_tree import BallTree
 from sklearn.neighbors._kd_tree import KDTree
-from sklearn.neighbors._base import _check_weights
+if not sklearn_check_version('1.2'):
+    from sklearn.neighbors._base import _check_weights
 from sklearn.neighbors._base import VALID_METRICS
 from sklearn.neighbors._classification import KNeighborsClassifier as \
     sklearn_KNeighborsClassifier
@@ -41,7 +38,7 @@ import numpy as np
 from scipy import sparse as sp
 
 
-if Version(sklearn_version) >= Version("0.24"):
+if sklearn_check_version("0.24"):
     class KNeighborsClassifier_(sklearn_KNeighborsClassifier):
         @_deprecate_positional_args
         def __init__(self, n_neighbors=5, *,
@@ -55,9 +52,9 @@ if Version(sklearn_version) >= Version("0.24"):
                 metric_params=metric_params,
                 n_jobs=n_jobs, **kwargs)
             self.weights = \
-                weights if Version(sklearn_version) >= Version("1.0") \
+                weights if sklearn_check_version("1.0") \
                 else _check_weights(weights)
-elif Version(sklearn_version) >= Version("0.22"):
+elif sklearn_check_version("0.22"):
     from sklearn.neighbors._base import SupervisedIntegerMixin as \
         BaseSupervisedIntegerMixin
 
@@ -110,7 +107,7 @@ class KNeighborsClassifier(KNeighborsClassifier_):
             n_jobs=n_jobs, **kwargs)
 
     def fit(self, X, y):
-        if Version(sklearn_version) >= Version("1.0"):
+        if sklearn_check_version("1.0"):
             self._check_feature_names(X, reset=True)
         if self.metric_params is not None and 'p' in self.metric_params:
             if self.p is not None:
@@ -226,7 +223,7 @@ class KNeighborsClassifier(KNeighborsClassifier_):
     @wrap_output_data
     def predict(self, X):
         check_is_fitted(self)
-        if Version(sklearn_version) >= Version("1.0"):
+        if sklearn_check_version("1.0"):
             self._check_feature_names(X, reset=False)
         return dispatch(self, 'neighbors.KNeighborsClassifier.predict', {
             'onedal': self.__class__._onedal_predict,
@@ -236,7 +233,7 @@ class KNeighborsClassifier(KNeighborsClassifier_):
     @wrap_output_data
     def predict_proba(self, X):
         check_is_fitted(self)
-        if Version(sklearn_version) >= Version("1.0"):
+        if sklearn_check_version("1.0"):
             self._check_feature_names(X, reset=False)
         return dispatch(self, 'neighbors.KNeighborsClassifier.predict_proba', {
             'onedal': self.__class__._onedal_predict_proba,
@@ -246,7 +243,7 @@ class KNeighborsClassifier(KNeighborsClassifier_):
     @wrap_output_data
     def kneighbors(self, X=None, n_neighbors=None, return_distance=True):
         check_is_fitted(self)
-        if Version(sklearn_version) >= Version("1.0"):
+        if sklearn_check_version("1.0"):
             self._check_feature_names(X, reset=False)
         return dispatch(self, 'neighbors.KNeighborsClassifier.kneighbors', {
             'onedal': self.__class__._onedal_kneighbors,
@@ -260,11 +257,11 @@ class KNeighborsClassifier(KNeighborsClassifier_):
 
         if _onedal_estimator is not None or getattr(self, '_tree', 0) is None and \
                 self._fit_method == 'kd_tree':
-            if Version(sklearn_version) >= Version("0.24"):
+            if sklearn_check_version("0.24"):
                 sklearn_NearestNeighbors.fit(self, self._fit_X, getattr(self, '_y', None))
             else:
                 sklearn_NearestNeighbors.fit(self, self._fit_X)
-        if Version(sklearn_version) >= Version("0.22"):
+        if sklearn_check_version("0.22"):
             result = sklearn_NearestNeighbors.radius_neighbors(
                 self, X, radius, return_distance, sort_results)
         else:
diff --git a/sklearnex/neighbors/knn_regression.py b/sklearnex/neighbors/knn_regression.py
index 1f6192c..93a3755 100755
--- a/sklearnex/neighbors/knn_regression.py
+++ b/sklearnex/neighbors/knn_regression.py
@@ -15,17 +15,14 @@
 # limitations under the License.
 #===============================================================================
 
-try:
-    from packaging.version import Version
-except ImportError:
-    from distutils.version import LooseVersion as Version
-from sklearn import __version__ as sklearn_version
+from daal4py.sklearn._utils import sklearn_check_version
 import warnings
 
 from sklearn.neighbors._base import NeighborsBase as sklearn_NeighborsBase
 from sklearn.neighbors._ball_tree import BallTree
 from sklearn.neighbors._kd_tree import KDTree
-from sklearn.neighbors._base import _check_weights
+if not sklearn_check_version('1.2'):
+    from sklearn.neighbors._base import _check_weights
 from sklearn.neighbors._base import VALID_METRICS
 from sklearn.neighbors._regression import KNeighborsRegressor as \
     sklearn_KNeighborsRegressor
@@ -41,7 +38,7 @@ import numpy as np
 from scipy import sparse as sp
 
 
-if Version(sklearn_version) >= Version("0.24"):
+if sklearn_check_version("0.24"):
     class KNeighborsRegressor_(sklearn_KNeighborsRegressor):
         @_deprecate_positional_args
         def __init__(self, n_neighbors=5, *,
@@ -55,9 +52,9 @@ if Version(sklearn_version) >= Version("0.24"):
                 metric_params=metric_params,
                 n_jobs=n_jobs, **kwargs)
             self.weights = \
-                weights if Version(sklearn_version) >= Version("1.0") \
+                weights if sklearn_check_version("1.0") \
                 else _check_weights(weights)
-elif Version(sklearn_version) >= Version("0.22"):
+elif sklearn_check_version("0.22"):
     from sklearn.neighbors._base import SupervisedFloatMixin as \
         BaseSupervisedFloatMixin
 
@@ -110,7 +107,7 @@ class KNeighborsRegressor(KNeighborsRegressor_):
             n_jobs=n_jobs, **kwargs)
 
     def fit(self, X, y):
-        if Version(sklearn_version) >= Version("1.0"):
+        if sklearn_check_version("1.0"):
             self._check_feature_names(X, reset=True)
         if self.metric_params is not None and 'p' in self.metric_params:
             if self.p is not None:
@@ -226,7 +223,7 @@ class KNeighborsRegressor(KNeighborsRegressor_):
     @wrap_output_data
     def predict(self, X):
         check_is_fitted(self)
-        if Version(sklearn_version) >= Version("1.0"):
+        if sklearn_check_version("1.0"):
             self._check_feature_names(X, reset=False)
         return dispatch(self, 'neighbors.KNeighborsRegressor.predict', {
             'onedal': self.__class__._onedal_predict,
@@ -236,7 +233,7 @@ class KNeighborsRegressor(KNeighborsRegressor_):
     @wrap_output_data
     def kneighbors(self, X=None, n_neighbors=None, return_distance=True):
         check_is_fitted(self)
-        if Version(sklearn_version) >= Version("1.0"):
+        if sklearn_check_version("1.0"):
             self._check_feature_names(X, reset=False)
         return dispatch(self, 'neighbors.KNeighborsRegressor.kneighbors', {
             'onedal': self.__class__._onedal_kneighbors,
@@ -250,11 +247,11 @@ class KNeighborsRegressor(KNeighborsRegressor_):
 
         if _onedal_estimator is not None or getattr(self, '_tree', 0) is None and \
                 self._fit_method == 'kd_tree':
-            if Version(sklearn_version) >= Version("0.24"):
+            if sklearn_check_version("0.24"):
                 sklearn_NearestNeighbors.fit(self, self._fit_X, getattr(self, '_y', None))
             else:
                 sklearn_NearestNeighbors.fit(self, self._fit_X)
-        if Version(sklearn_version) >= Version("0.22"):
+        if sklearn_check_version("0.22"):
             result = sklearn_NearestNeighbors.radius_neighbors(
                 self, X, radius, return_distance, sort_results)
         else:
diff --git a/sklearnex/neighbors/knn_unsupervised.py b/sklearnex/neighbors/knn_unsupervised.py
index e5039a2..34a8773 100755
--- a/sklearnex/neighbors/knn_unsupervised.py
+++ b/sklearnex/neighbors/knn_unsupervised.py
@@ -20,6 +20,7 @@ try:
 except ImportError:
     from distutils.version import LooseVersion as Version
 from sklearn import __version__ as sklearn_version
+from daal4py.sklearn._utils import sklearn_check_version
 import warnings
 
 from sklearn.neighbors._base import NeighborsBase as sklearn_NeighborsBase
@@ -39,7 +40,7 @@ import numpy as np
 from scipy import sparse as sp
 
 
-if Version(sklearn_version) >= Version("0.22") and \
+if sklearn_check_version("0.22") and \
    Version(sklearn_version) < Version("0.23"):
     class NearestNeighbors_(sklearn_NearestNeighbors):
         def __init__(self, n_neighbors=5, radius=1.0,
@@ -78,7 +79,7 @@ class NearestNeighbors(NearestNeighbors_):
             metric_params=metric_params, n_jobs=n_jobs)
 
     def fit(self, X, y=None):
-        if Version(sklearn_version) >= Version("1.0"):
+        if sklearn_check_version("1.0"):
             self._check_feature_names(X, reset=True)
         if self.metric_params is not None and 'p' in self.metric_params:
             if self.p is not None:
@@ -194,7 +195,7 @@ class NearestNeighbors(NearestNeighbors_):
     @wrap_output_data
     def kneighbors(self, X=None, n_neighbors=None, return_distance=True):
         check_is_fitted(self)
-        if Version(sklearn_version) >= Version("1.0"):
+        if sklearn_check_version("1.0"):
             self._check_feature_names(X, reset=False)
         return dispatch(self, 'neighbors.NearestNeighbors.kneighbors', {
             'onedal': self.__class__._onedal_kneighbors,
@@ -208,11 +209,11 @@ class NearestNeighbors(NearestNeighbors_):
 
         if _onedal_estimator is not None or getattr(self, '_tree', 0) is None and \
                 self._fit_method == 'kd_tree':
-            if Version(sklearn_version) >= Version("0.24"):
+            if sklearn_check_version("0.24"):
                 sklearn_NearestNeighbors.fit(self, self._fit_X, getattr(self, '_y', None))
             else:
                 sklearn_NearestNeighbors.fit(self, self._fit_X)
-        if Version(sklearn_version) >= Version("0.22"):
+        if sklearn_check_version("0.22"):
             result = sklearn_NearestNeighbors.radius_neighbors(
                 self, X, radius, return_distance, sort_results)
         else:
diff --git a/sklearnex/tests/test_memory_usage.py b/sklearnex/tests/test_memory_usage.py
index 9be9e88..a9c12b2 100644
--- a/sklearnex/tests/test_memory_usage.py
+++ b/sklearnex/tests/test_memory_usage.py
@@ -146,6 +146,8 @@ def split_train_inference(kf, x, y, estimator):
         elif isinstance(x, pd.core.frame.DataFrame):
             x_train, x_test = x.iloc[train_index], x.iloc[test_index]
             y_train, y_test = y.iloc[train_index], y.iloc[test_index]
+        # TODO: add parameters for all estimators to prevent
+        # fallback to stock scikit-learn with default parameters
         alg = estimator()
         alg.fit(x_train, y_train)
         if hasattr(alg, 'predict'):
